import librosa
import numpy as np
import openai
import wave
import os
import logging
import matplotlib.pyplot as plt
from dotenv import load_dotenv

load_dotenv()

# ðŸŽ¼ Æ¯á»›c lÆ°á»£ng há»£p Ã¢m tá»« vector chroma
def estimate_chord(chroma_vector):
    pitch_classes = ['C', 'C#', 'D', 'D#', 'E', 'F',
                     'F#', 'G', 'G#', 'A', 'A#', 'B']

    chord_templates = {
        "":     [0, 4, 7],        # major
        "m":    [0, 3, 7],        # minor
        "7":    [0, 4, 7, 10],    # dominant 7
        "maj7": [0, 4, 7, 11],    # major 7
        "m7":   [0, 3, 7, 10],    # minor 7
        "dim7": [0, 3, 6, 9]      # diminished 7
    }

    best_score = -1
    best_chord = None

    for i in range(12):
        for suffix, intervals in chord_templates.items():
            template = np.zeros(12)
            for interval in intervals:
                note_index = (i + interval) % 12
                template[note_index] = 1

            score = np.dot(chroma_vector, template)
            if score > best_score:
                best_score = score
                best_chord = pitch_classes[i] + suffix

    return best_chord

# ðŸŽ¼ TÃ¡ch há»£p Ã¢m tá»« Ä‘oáº¡n
def extract_chords_from_frames(y, sr, frame_duration=0.5):
    frame_length = int(sr * frame_duration)
    total_frames = int(len(y) / frame_length)
    chords = []

    for i in range(total_frames):
        start = i * frame_length
        end = start + frame_length
        y_frame = y[start:end]

        if len(y_frame) < frame_length:
            break  # Bá» qua Ä‘oáº¡n cuá»‘i náº¿u khÃ´ng Ä‘á»§

        # DÃ¹ng chroma_cens thay cho chroma_cqt
        chroma = librosa.feature.chroma_cens(y=y_frame, sr=sr)
        avg_chroma = np.median(chroma, axis=1)  # DÃ¹ng median thay vÃ¬ mean

        chord_name = estimate_chord(avg_chroma)
        chords.append(chord_name)

    return chords

# ðŸ” Nháº­n diá»‡n vÃ²ng hÃ²a Ã¢m phá»• biáº¿n
def detect_common_progressions(chords):
    common_patterns = {
        "Iâ€“IVâ€“V": ["C", "F", "G"],
        "viâ€“IVâ€“Iâ€“V": ["Am", "F", "C", "G"],
        "iiâ€“Vâ€“I": ["Dm", "G", "C"]
    }

    for name, pattern in common_patterns.items():
        for i in range(len(chords) - len(pattern) + 1):
            if chords[i:i+len(pattern)] == pattern:
                return f"ðŸŽµ PhÃ¡t hiá»‡n vÃ²ng hÃ²a Ã¢m: {name} ({' - '.join(pattern)})"
    return "âš ï¸ KhÃ´ng phÃ¡t hiá»‡n vÃ²ng hÃ²a Ã¢m phá»• biáº¿n."

# ðŸ§  Gá»i GPT Ä‘á»ƒ phÃ¢n tÃ­ch lá»i nháº¡c
def analyze_lyrics(lyrics):
    if not os.getenv("OPENAI_API_KEY"):
        return "âŒ Thiáº¿u biáº¿n mÃ´i trÆ°á»ng OPENAI_API_KEY."

    client = openai.OpenAI()

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia Ã¢m nháº¡c."},
            {"role": "user", "content": f"PhÃ¢n tÃ­ch lá»i bÃ i hÃ¡t sau vÃ  nháº­n diá»‡n chá»§ Ä‘á», cáº£m xÃºc vÃ  vÃ²ng hÃ²a Ã¢m náº¿u cÃ³:\n{lyrics}"}
        ],
        temperature=0.7,
        max_tokens=300,
    )
    return response.choices[0].message.content.strip()

# ðŸ“Š Váº½ biá»ƒu Ä‘á»“ há»£p Ã¢m
def plot_chord_progression(chords, frame_duration=0.5):
    times = [i * frame_duration for i in range(len(chords))]
    fig, ax = plt.subplots(figsize=(12, 4))
    ax.bar(times, [1]*len(chords), width=frame_duration*0.8)

    for i, chord in enumerate(chords):
        ax.text(times[i], 1.05, chord, ha='center', va='bottom', fontsize=10)

    ax.set_yticks([])
    ax.set_xlabel("Thá»i gian (giÃ¢y)")
    ax.set_title("ðŸŽµ Biá»ƒu Ä‘á»“ há»£p Ã¢m theo thá»i gian")
    plt.tight_layout()
    return fig